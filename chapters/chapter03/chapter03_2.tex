\section{Evaluation von Azure Diensten} \label{sec:evaluation}
Im Folgenden sollen einige Azure Dienste, die im Rahmen einer Literaturrecherche ausgewählt wurden, evaluiert werden, mit dem Ziel, aus den gewonnen Erkenntnissen ein BI-System zu entwerfen, dass die festgelegten Anforderungen erfüllt. Dienste, die innerhalb der nächsten fünf Jahre eingestellt werden, wurden vorab ausgeschlossen und Funktionen, die sich noch in der Preview befinden, werden als nicht existent betrachtet. 

Die untersuchten Dienste werden jeweils kurz vorgestellt und es wird auf die wichtigsten Funktionen, Vor- und Nachteile eingegangen, die in bestehender Literatur und den jeweiligen Dokumentationen gefunden werden konnten. Eine Einschätzung der Dienste in Bezug auf die Anforderungen kann in Tabelle~\ref{table:eva} gefunden werden.

\subsection{Azure Table Storage} \label{sec:grundlagen:azure_dienste:tableStorage}
Der NoSQL-Dienst Table Storage gehört zu den günstigsten Speichermöglichkeiten in Azure. Die Daten werden als Schlüssel/Wert-Paare in Tabellen gespeichert, diese sind jedoch nicht relational und können nicht miteinander gejoint werden. Eine Tabelle besteht aus einer oder mehreren Partitionen und eine Partition besteht aus ein oder mehreren Zeilen. Auf jeden Tabelleneintrag kann über einen eindeutigen Schlüssel zugegriffen werden. Dieser ist eine Kombination aus Partitions- und Zeilen-Schlüssel. Die Partitionierung über mehrere Server ermöglicht es Datenmengen im dreistelligen Terabyte Bereich zu speichern \cite{reagan_web_2018}.

Die gespeicherten Daten werden standardmäßig mit 256-AES verschlüsselt und sind dadurch ohne zusätzlichen Aufwand geschützt \cite{soh_microsoft_2020}. 

Zum Abfragen der Daten gibt es verschiedene Möglichkeiten. Am schnellsten ist die \textit{point query} bei der Partitions- und Zeilen-Schlüssel angegeben werden. Daneben gibt es verschiedene Bereichsabfragen, die entweder die Tabellen einer Partition, mehrerer oder aller Partitionen scannen. Innerhalb der gleichen Partition werden außerdem Transaktionen unterstützt.

Die \textit{Azure Storage Client Library} bietet die Möglichkeit fehlgeschlagene Abfragen, zum Beispiel wegen eines Server-Timeouts, automatisch zu wiederholen. Alternativ kann beim Zugriff über REST API eine eigene Wiederholungslogik bei Fehlern implementiert werden.

Die größte Schwäche von Table Storage ist, dass nur auf den Partitions- und Zeilen-Schlüssel ein Index angelegt wird. Performante Abfragen auf andere Attribute als die Schlüssel sind daher nicht möglich \cite{reagan_web_2018}.

\textbf{Fazit}: Table Storage ist günstig, schnell und kann problemlos die Anforderungen an die zu speichernde Datenmenge erfüllen. Jedoch werden für die Auswertungen im aktuellen System sowohl Tabellen-Joins als auch Abfragen von Attributen, die nicht der indexierte Schlüssel sind, verwendet. Eine daraus folgende Einschränkung ist, dass das \textit{Recht auf Vergessenwerden} nur umgesetzt werden kann, wenn eine eindeutige Kunden- bzw. Personenkennzeichnung als Schlüssel verwendet wird.

Daher ist der Table Storage hier nicht geeignet, um als alleinstehende Speicherlösung genutzt zu werden. Er könnte jedoch als Ergänzung zum Kosten sparen eingesetzt werden \cite[vgl.][]{reagan_web_2018}. 

\subsection{Azure SQL} \label{sec:grundlagen:azure_dienste:sql}
Azure SQL ist eine relationale Datenbank, die vergleichbar mit dem on-premise Microsoft SQL Server, der im aktuellen BI-System genutzt wird, ist. Bei einer Migration hat dies den Vorteil, dass die meisten SQL-Queries zum Laden und Transformieren von Daten, ohne Anpassung verwendet werden können. Der Einsatz von Azure SQL empfiehlt sich besonders, wenn hochgradig relationale Daten vorliegen. Die alternativen relationalen Datenbanken in Azure, wie MySQL und PostgresSQL, werden in dieser Arbeit nicht betrachtet.

Ruhende Daten werden von Azure SQL automatisch verschlüsselt. Die Konsistenz der gespeicherten Daten kann durch die Wahl eines geeigneten Datenmodells erreicht werden. Für relationale Datenbanken wird die Normalisierung der Daten empfohlen. Das bedeutet, dass es keine doppelten Werte gibt und die Tabellen in einer angemessenen Beziehung zueinander stehen.

Die Azure SQL Datenbank wird in einer virtualisierten Umgebung gehostet. Die Performance ist von DTUs abhängig, welche eine Kombination aus CPU, Speicher und der Anzahl an unterstützten Lese-/ und Schreib-Prozessen von Daten und auf dem Transaktionsprotokoll sind. Durch eine Drosselung schlagen Abfragen fehl, wenn die festgelegten Limits der SQL-Datenbank-Instanz überschritten werden. Es gibt jedoch Möglichkeiten dieses Problem durch eine automatische Wiederholungslogik bei den Abfragen zu lösen. Die DTUs sowie die Speicherkapazität sind abhängig von der gewählten Preisstufe.


Es gibt zwei Möglichkeiten, die Performance zu verbessern. Solange die oberste Preisstufe noch nicht erreicht wurde, ist das Erhöhen dieser der einfachste Weg, Leistung und Speicherkapazität zu steigern. Azure führt dazu alle notwendigen Schritte zum Ändern der Datenbank automatisch im Hintergrund aus. Eine automatische Skalierung nach Bedarf wird jedoch nicht unterstützt. Die zweite deutlich komplexere Option ist die horizontale Partitionierung der Daten auf mehrere SQL-Instanzen (Sharding). \cite{reagan_web_2018}

\textbf{Fazit}: Der größte Vorteil von Azure SQL ist, dass alle zu migrierenden Daten bereits in einem relationalen Datenmodell vorliegen. Auch die festgelegten Leistungsanforderungen können mit einer einzigen Instanz abgedeckt werden. Daher sind die potenziellen Probleme und hohen Kosten, die beim Sharding entstehen könnten, hier irrelevant.

\subsection{Azure Cosmos DB} \label{sec:grundlagen:azure_dienste:cosmosDB}
Bei diesem Dienst handelt es sich um eine NoSQL-Datenbank, die mehrere Datenmodelle, wie Schlüssel-Wert, Dokument oder Graph, unterstützt. Intern werden die Daten als Dokumente im JSON-Format gespeichert. Solange die Daten sich in Ruhe befinden, sind sie verschlüsselt.

Dabei werden verschiedene APIs zum Zugriff auf die Daten angeboten, wie unter anderem DocumentDB SQL (bietet gängige SQL-Abfragefunktionen), MongoDB, Apache Casandra, Graph und Table (bietet gleiche Funktionalität wie Azure Table). Auch das Konsistenzniveau der Daten kann aus fünf Ebenen, von starker bis eventueller Konsistenz, gewählt werden.

Das Feature \textit{Turnkey Global Distribution} beschreibt die automatisierbare Replikation über verschiedene Regionen. Dadurch können weltweit geringe Latenzzeiten gewährleistet werden. Die einzige notwendige Konfiguration ist dabei die Auswahl der gewünschten Regionen, alles andere wird automatisch von Azure übernommen.

Sowohl der Durchsatz, der in \acp{ru} gemessen wird, als auch die Speicherkapazität kann dynamisch nach Bedarf skaliert werden. Davon sind auch die entstehenden Kosten abhängig.
\cite{guay_paz_microsoft_2018}\cite{mrzyglod_hands-azure_2018}

\textbf{Fazit}: Besonders das gleichzeitige Verwenden mehrerer Datenmodelle macht aus Cosmos DB eine interessante Möglichkeit für ein BI-System. Für alle gespeicherten Daten, kann das Modell verwendet werden, dass für den benötigten Anwendungsfall optimal ist.

\subsection{Azure Data Lake Gen 2} \label{sec:grundlagen:azure_dienste:dataLake}
\acp{blob} sind unstrukturierte Dateien, die sich nicht zum Speichern in einer Datenbank eignen. Eine kostengünstige Möglichkeit, diese in der Cloud abzulegen, ist der Blob Storage. Auf diesen Dienst baut auch der Data Lake Gen 2 auf, welcher für die Analyse der gespeicherten Daten spezialisiert wurde. Organisieren lassen sich die Dateien in einer hierarchischen Ordnerstruktur \cite{soh_microsoft_2020}.

Der Data Lake ist in verschiedene Schichten eingeteilt. Die oberste Ebene bezieht sich auf die Umgebung, wie zum Beispiel Entwicklung, Test oder Produktion. Innerhalb einer Umgebung kann es einen oder mehrere Storage Accounts geben. In dieser Schicht können Einstellungen, wie die Leistungsstufe, oder die Replikationsmethode gesetzt werden. Die Leistungsstufe bestimmt unter anderem, ob ein günstiger Magnetspeicher oder Solid-State-Festplatten für schneller Lese-/Schreib-Prozesse, verwendet wird. Die nächste Schicht ist das Dateisystem in dem sich die Verzeichnisse und Dateien befinden.

Azure Data Lake Gen 2 verschlüsselt die Daten, während sie sich in Ruhe befinden oder bewegt werden, automatisch.

Der Zugriff auf den Data Lake kann auf der Storage Account Ebene über \ac{rbac} gesteuert werden. Für eine granulare Zugriffskontrolle können auf der Verzeichnis/Ordner-Ebene POSIX-Artige Zugriffssteuerungslisten (ACLs) konfiguriert werden \cite{lesteve_definitive_2021}.

\subsection{Azure Logic Apps} \label{sec:grundlagen:azure_dienste:logicApps}
Mit diesem Dienst können automatisierte Workflows zur Datenintegration erstellt werden. Die Workflows sind sehr anpassungsfähig und es steht eine Vielzahl an vorgefertigten Konnektoren zur Verfügung \cite{kumar_serverless_2019}.

Ein Logic Apps Workflow besteht aus zusammenhängenden Aktivitäten, in einer bestimmten Reihenfolge. Die erste Aktivität ist immer ein Trigger, der die Ausführung des Workflows startet. Alle weiteren Aktivitäten sind Aktionen, die jeweils eine einzige Aufgabe erfüllen und nacheinander abgearbeitet werden. Hier können auch die Konnektoren verwendet werden, mit denen eine Verbindung zu externen Ressourcen aufgebaut werden kann. Die Verbindung ist zu Cloud und on-premise Ressourcen möglich. Die Workflows können über eine Drag-and-Drop-Benutzeroberfläche erstellt werden \cite{modi_azure_2020}.

Auch Datentransformationen, wie mathematische Berechnungen und Textmanipulationen, sind in einem Workflow mit möglich. Dazu werden \ac{dax} Funktionen verwendet, welche mit Excel Formeln vergleichbar sind \cite{bennett_enterprise_2021}.

Beim Eintreten von Fehlern werden vorher festgelegte Wiederholungsrichtlinien ausgeführt. Es ist ebenfalls möglich, nach einem aufgetretenen Fehler, mit einer alternativen Aktion fortzufahren, um zum Beispiel eine zuständige Person per E-Mail zu informieren \cite{fan_handle_2021}.

Für eine ausreichende Sicherheit werden die Daten während der Übertragung mit TLS und im Ruhezustand verschlüsselt. Auch die Authentifizierung mit \ac{aad} und Autorisierung durch \ac{rbac} wird unterstützt \cite{baldwin_azure_2021-1}. 

\textbf{Fazit}: Mit den angebotenen Konnektoren von Logic Apps, können alle hier benötigten Quellsysteme integriert werden \cite[vgl.][]{fan_verwaltete_2021}. Vor dem Speichern können die Daten bei Bedarf verarbeitet werden. Dieser Dienst ist außerdem performant und benötigt für die meisten Ausführungen nur wenige Sekunden \cite{bennett_enterprise_2021}. Durch die Manipulationsmöglichkeiten wäre auch ein Verarbeitungsschritt möglich, in dem nach festen Regeln Attribute zur Datenklassifikation ergänzt werden.

\subsection{Azure Data Factory} \label{sec:grundlagen:azure_dienste:dataFactory}
\ac{adf} ist ein Datenintegrationsdienst, der für on-premise und Cloud Systeme genutzt werden kann. Der Dienst wurde speziell für den gemeinsamen Einsatz mit anderen Diensten entwickelt und übernimmt dabei die Bewegung, Transformation und Verarbeitung der Daten, zwischen den unterschiedlichen Systemen \cite{klein_iot_2017}.

Der ETL-Prozess wird als Pipeline implementiert. Eine Pipeline ist eine Sammlung von Aktivitäten zum Bewegen und Transformieren von Daten. 

Die \textit{Azure Data Factory User Experience} bietet eine webbasierte IDE für die visuelle Erstellung, Planung und Überwachung von Pipelines. Das \textit{Copy Data tool} führt einen schrittweise durch die Erstellung einer neuen Pipeline, die Daten von einer Ressource zu einer anderen verschiebt. Dabei werden unter anderen Datenspeicher- und Computedienste mit der \ac{adf} verknüpft. Die Trennung dieser hat den Vorteil, dass beides unabhängig voneinander nach Bedarf skaliert werden kann.

\ac{adf} besitzt keinen eigenen Speicher, sondern verwendet ausschließlich die verknüpften Dienste. Ein verknüpfter Dienst besteht aus den notwendigen Metadaten für eine Verbindung, beispielsweise die Verbindungszeichenfolge für eine Datenbank. Metadaten, die konkrete Objekte im verknüpften Speicherdienst beschreiben, werden als Dataset definiert. 

Für die Verarbeitung der Daten können verknüpfte Dienste, wie Azure Databricks oder HDInsight verwendet werden. Alternativ bietet eine \textit{integration runtime} Zugang zu internen Rechenressourcen innerhalb von \ac{adf} \cite{swinbank_your_2021}.

\textbf{Fazit}: \ac{adf} könnte im neuen BI-System für die Datenintegration genutzt werden. Im Gegensatz zu Logic Apps setzt dieser Dienst auf die Zusammenarbeit mit weiteren Azure Ressourcen, um den ETL-Prozess durchzuführen, was zu einem komplexeren Gesamtsystem führen könnte.

\subsection{Azure HDInsight} \label{sec:grundlagen:azure_dienste:hdInsight}
Bei HDInsight handelt es sich um Apache Hadoop, als vollständig verwalteten Cloud Dienst. Hadoop ist eine Sammlung von Open~=Source Komponenten, die für die verteilte Verarbeitung und Analyse großer Datensätze konzipiert wurden. Dieser Dienst ist vergleichsweise komplex, da individuelle Konfigurationen notwendig sind \cite{klein_iot_2017}.

\subsection{Azure Databricks} \label{sec:grundlagen:azure_dienste:databricks}
Databricks setzt Apache Spark als Cloud-Dienst um. Letzteres ist ein weiteres Open-Source Framework für die Big Data Analyse. Im Gegensatz zu Hadoop werden die Daten zur schnelleren Verarbeitung im Arbeitsspeicher vorgehalten. Ein weiteres Feature ist die mögliche Echtzeitverarbeitung von Streaming-Daten \cite{soh_data_2020}.

\subsection{Azure Machine Learning} \label{sec:grundlagen:azure_dienste:machineLearning}
Azure Machine Learning ist eine Sammlung von Diensten und Tools für maschinelles Lernen in der Cloud \cite{soh_data_2020}.

\subsection{Azure Synapse Analytics} \label{sec:grundlagen:azure_dienste:synapseAnalytics}
Synapse Analytics vereint die Datenintegration, Data Warehousing und Big Data Analysen zu einem Dienst. Die Datenintegration verwendet die Data Factory, wird hier aber als \textit{Synapse Pipeline} bezeichnet. Die integrierten Daten können unabhängig von ihrer Struktur gespeichert werden. Durch die Unterstützung von Apache Spark wird die Analyse von großen Datenmengen ermöglicht \cite{shiyal_beginning_2021}.

\subsection{Azure Analysis Services} \label{sec:grundlagen:azure_dienste:analysisServices}
Dieser Dienst kann als Semantik-Schicht zwischen \ac{dwh} und Endnutzer agieren. In dieser Funktion können beispielsweise Spalten umbenannt oder irrelevante Werte ausgeblendet werden, damit übersichtliche Reports entstehen. Auch eine granulare Implementierung von \ac{rbac} wird so vereinfacht. Der Dienst kann jedoch nur mit tabellarischen Datenmodellen umgehen, nicht mit multidimensionalen \cite{how_beyond_2020}.

\subsection{Power BI} \label{sec:grundlagen:azure_dienste:powerBI}
Power BI ist Microsofts Dienst zum Visualisieren von Daten. Gleichzeitig werden \ac{etl} Prozesse und Datenanalyse auf Basis von Analysis Services bereitgestellt. Mit Power BI können vorgefertigte und Self-Service Reports erstellt werden \cite{how_beyond_2020}.

\subsection{Azure Purview} \label{sec:grundlagen:azure_dienste:purview}
Purview dient der zentralen Verwaltung der Datengovernance, für Cloud und on-premise Umgebungen. Dabei soll die Datenherkunft leicht nachvollziehbar werden. Die Metadaten zu den Quellen können um Tags und Beschreibungen erweitert werden. Der Dienst kann über die Benutzeroberfläche Purview Studio verwaltet werden. Der Zugriff auf dieses und die Informationen wird über Rollen und das \ac{aad} kontrolliert.

Im Purview Studio können Benutzer neue Datenquellen registriert werden, damit zusätzliche Metadaten zu diesen verwaltet werden können. Durch das Erstellen einer \textit{Scan Rule} können die Datenquellen gescannt werden, wobei die Daten nach vorher definierten Regeln klassifiziert werden. Der Scan kann einmalig oder wiederholt nach Zeitplan ausgeführt werden \cite{lesteve_definitive_2021}. Dadurch wird die Nachvollziehbarkeit der Datenherkunft ermöglicht. Diese umfasst den Ursprung, aber auch alle Verarbeitungen und Bewegungen der Daten \cite{riscutia_data_2021}.

Auch die Übertragung und Speicherung der Daten geschieht in Purview verschlüsselt \cite{baldwin_azure_2021}. 

\textbf{Fazit}: Purview bietet einige Funktionen, die helfen, die Sicherheit und den Datenschutz der BI-Lösung zu verbessern. Purview kann verwendet werden, um die Daten in regelmäßigen Scans zu klassifizieren und dabei den vollständigen Datenfluss zu dokumentieren. Durch die Erweiterung der Metadaten um bestimmte Tags kann Purview zusätzlich dabei helfen, auf Anfrage alle Daten eines Kunden zu löschen.