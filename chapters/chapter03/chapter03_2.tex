\section{Evaluation von Azure Diensten} \label{sec:evaluation}
Im Folgenden sollen einige Azure Dienste, die im Rahmen einer Literaturrecherche ausgewählt wurden, evaluiert werden, mit dem Ziel, aus den gewonnen Erkenntnissen ein BI-System zu entwerfen, dass die festgelegten Anforderungen erfüllt. Dienste, die innerhalb der nächsten fünf Jahre eingestellt werden, wurden vorab ausgeschlossen und Funktionen, die sich noch in der Preview befinden, werden als nicht existent betrachtet. 

Die untersuchten Dienste werden jeweils kurz vorgestellt und es wird auf die wichtigsten Funktionen, Vor- und Nachteile eingegangen, die in bestehender Literatur und den jeweiligen Dokumentationen gefunden werden konnten.


%===========================================================================================================================
\subsection{Azure Table Storage} \label{sec:grundlagen:azure_dienste:tableStorage}
Der NoSQL-Dienst Table Storage gehört zu den günstigsten Speichermöglichkeiten in Azure. Die Daten werden als Schlüssel/Wert-Paare in Tabellen gespeichert, diese sind jedoch nicht relational und können nicht miteinander gejoint werden. Eine Tabelle besteht aus einer oder mehreren Partitionen und eine Partition besteht aus ein oder mehreren Zeilen. Auf jeden Tabelleneintrag kann über einen eindeutigen Schlüssel zugegriffen werden. Dieser ist eine Kombination aus Partitions- und Zeilen-Schlüssel. Die Partitionierung über mehrere Server ermöglicht es Datenmengen im dreistelligen Terabyte Bereich zu speichern \cite{reagan_web_2018}.

Die gespeicherten Daten werden standardmäßig mit 256-AES verschlüsselt und sind dadurch ohne zusätzlichen Aufwand geschützt \cite{soh_microsoft_2020}. 

Zum Abfragen der Daten gibt es verschiedene Möglichkeiten. Am schnellsten ist die \textit{point query} bei der Partitions- und Zeilen-Schlüssel angegeben werden. Daneben gibt es verschiedene Bereichsabfragen, die entweder die Tabellen einer Partition, mehrerer oder aller Partitionen scannen. Innerhalb der gleichen Partition werden außerdem Transaktionen unterstützt.

Die \textit{Azure Storage Client Library} bietet die Möglichkeit fehlgeschlagene Abfragen, zum Beispiel wegen eines Server-Timeouts, automatisch zu wiederholen. Alternativ kann beim Zugriff über REST API eine eigene Wiederholungslogik bei Fehlern implementiert werden.

Die größte Schwäche von Table Storage ist, dass nur auf den Partitions- und Zeilen-Schlüssel ein Index angelegt wird. Performante Abfragen auf andere Attribute als die Schlüssel sind daher nicht möglich \cite{reagan_web_2018}.

\textbf{Fazit}: Table Storage ist günstig, schnell und kann problemlos die Anforderungen an die zu speichernde Datenmenge erfüllen. Jedoch werden für die Auswertungen im aktuellen System sowohl Tabellen-Joins als auch Abfragen von Attributen, die nicht der indexierte Schlüssel sind, verwendet. Eine daraus folgende Einschränkung ist, dass das \textit{Recht auf Vergessenwerden} nur umgesetzt werden kann, wenn eine eindeutige Kunden- bzw. Personenkennzeichnung als Schlüssel verwendet wird.

Daher ist der Table Storage hier nicht geeignet, um als alleinstehende Speicherlösung genutzt zu werden. Er könnte jedoch als Ergänzung zum Kosten sparen eingesetzt werden \cite[vgl.][]{reagan_web_2018}. 


%===========================================================================================================================
\subsection{Azure SQL} \label{sec:grundlagen:azure_dienste:sql}
Azure SQL ist eine Produktfamilie von relationalen Datenbanken, die basierend auf Abstraktion und Zugriff in Betriebssystem-, Server- und Datenbankebene eingeteilt werden kann:

\begin{itemize}
\item \textbf{SQL Server auf Azure-VMs} stellt einen Microsoft SQL Server auf Betriebssystemebene bereit. Es handelt sich um eine virtuelle Maschine, auf der ein SQL Server installiert ist. Dadurch hat der Nutzer auf der einen Seite mehr Kontrolle, auf der anderen aber auch mehr Verantwortung und Konfigurationsaufwand. 
\item Wird nur die Kontrolle über den SQL-Server, aber nicht über das Betriebssystem benötigt, eignet sich die \textbf{Verwaltete Azure SQL-Instanz}. Diese bietet nahezu alle Features einer normalen SQL-Server-Instanz. Eines dieser Features ist zum Beispiel Machine Learning mit R oder Python auf die Daten anzuwenden \cite{ericson_was_2021}.
\item Die \textbf{Azure SQL-Datenbank} ist eine relationale Datenbank in der Cloud die ohne Verwaltungsaufwand genutzt werden kann. Es handelt sich damit um die höchste Abstraktionsebene, die dann eingesetzt werden kann, wenn weder Kontrolle über die Instanz noch Features auf Serverebene benötigt werden.
\end{itemize}
\cite{mauri_azure_2021}

Die Abfragesprache T-SQL bietet einige Möglichkeiten zum Transformieren und Auswerten der Daten, wie zum Beispiel die Umwandlung in einen anderen Datentyp oder die Durchführung von mathematischen Berechnungen. Für den Umgang mit möglicherweise auftretenden Fehlern kann in T-SQL eine Try/Catch-Logik genutzt werden \cite[vgl.][]{kellenberger_beginning_2021}.

Ruhende Daten werden von Azure SQL-Datenbank automatisch verschlüsselt. Die Konsistenz der gespeicherten Daten kann durch die Wahl eines geeigneten Datenmodells erreicht werden \cite{reagan_web_2018}. Auch die Autorisierung über das \ac{aad} mit \ac{rbac} wird von Azure SQL unterstützt \cite{wolter_authorize_2021}.

Es gibt zwei Möglichkeiten, die Performance bei Bedarf zu verbessern. Solange die oberste Preisstufe noch nicht erreicht wurde, ist das Erhöhen dieser der einfachste Weg, Leistung und Speicherkapazität zu steigern. Azure führt dazu alle notwendigen Schritte zum Ändern der Datenbank automatisch im Hintergrund aus. Eine automatische Skalierung nach Bedarf wird jedoch nicht unterstützt. Die zweite deutlich komplexere Option ist die horizontale Partitionierung der Daten auf mehrere SQL-Instanzen (Sharding) \cite{reagan_web_2018}.

\textbf{Fazit}: Der größte Vorteil von Azure SQL ist, dass alle zu migrierenden Daten bereits in einem relationalen Datenmodell vorliegen. Zusätzlich eignet sich die T-SQL für einfache Datenverarbeitungen, wobei auch hier die meisten Skripte aus dem Bestandssystem übernommen werden könnten.

Von den drei Azure SQL Diensten werden nur die verwaltete SQL-Instanz und die SQL-Datenbank für das neue BI-System in Erwägung gezogen, da die Kontrolle auf Betriebssystemebene nicht benötigt wird. Welcher dieser beiden Dienste besser geeignet ist, hängt besonders davon ab, wie die zusätzlichen Funktionen der verwalteten SQL-Instanz gegenüber der SQL-Datenbank in Verbindung mit zusätzlichen Diensten zu werten sind. 


%===========================================================================================================================
\subsection{Azure Cosmos DB} \label{sec:grundlagen:azure_dienste:cosmosDB}
Bei diesem Dienst handelt es sich um eine NoSQL-Datenbank, die mehrere Datenmodelle, wie Schlüssel-Wert, Dokument oder Graph, unterstützt. Intern werden die Daten als Dokumente im JSON-Format gespeichert. Solange die Daten sich in Ruhe befinden, sind sie verschlüsselt.

Dabei werden verschiedene APIs zum Zugriff auf die Daten angeboten, wie unter anderem DocumentDB SQL (bietet gängige SQL-Abfragefunktionen), MongoDB, Apache Casandra, Graph und Table (bietet gleiche Funktionalität wie Azure Table). Auch das Konsistenzniveau der Daten kann aus fünf Ebenen, von starker bis eventueller Konsistenz, gewählt werden.

Das Feature \textit{Turnkey Global Distribution} beschreibt die automatisierbare Replikation über verschiedene Regionen. Dadurch können weltweit geringe Latenzzeiten gewährleistet werden. Die einzige notwendige Konfiguration ist dabei die Auswahl der gewünschten Regionen, alles andere wird automatisch von Azure übernommen.

Sowohl der Durchsatz, der in \acp{ru} gemessen wird, als auch die Speicherkapazität kann dynamisch nach Bedarf skaliert werden. Davon sind auch die entstehenden Kosten abhängig.
\cite{guay_paz_microsoft_2018}\cite{mrzyglod_hands-azure_2018}

Der Zugriff auf Cosmos DB kann über \ac{aad} und \ac{rbac} kontrolliert werden \cite{weiss_azure_2021}.

\textbf{Fazit}: Besonders das gleichzeitige Verwenden mehrerer Datenmodelle macht aus Cosmos DB eine interessante Möglichkeit für ein BI-System. Für alle gespeicherten Daten, kann das Modell verwendet werden, dass für den benötigten Anwendungsfall optimal ist. Von der gewählten API sind auch die Möglichkeiten zur Datenfernverarbeitung mit diesem Dienst abhängig. Während die Table API dafür ungeeignet ist, sind mit der SQL-Artigen DocumentDB Transformationen und Auswertungen möglich.


%===========================================================================================================================
\subsection{Azure Data Lake Gen 2} \label{sec:grundlagen:azure_dienste:dataLake}
\acp{blob} sind unstrukturierte Dateien, die sich nicht zum Speichern in einer Datenbank eignen. Eine kostengünstige Möglichkeit, diese in der Cloud abzulegen, ist der Blob Storage. Auf diesen Dienst baut auch der Data Lake Gen 2 auf, welcher für die Analyse der gespeicherten Daten spezialisiert wurde. Organisieren lassen sich die Dateien in einer hierarchischen Ordnerstruktur \cite{soh_microsoft_2020}.

Der Data Lake ist in verschiedene Schichten eingeteilt. Die oberste Ebene bezieht sich auf die Umgebung, wie zum Beispiel Entwicklung, Test oder Produktion. Innerhalb einer Umgebung kann es einen oder mehrere Storage Accounts geben. In dieser Schicht können Einstellungen, wie die Leistungsstufe, oder die Replikationsmethode gesetzt werden. Die Leistungsstufe bestimmt unter anderem, ob ein günstiger Magnetspeicher oder Solid-State-Festplatten für schneller Lese-/Schreib-Prozesse, verwendet wird. Die nächste Schicht ist das Dateisystem in dem sich die Verzeichnisse und Dateien befinden.

Azure Data Lake Gen 2 verschlüsselt die Daten, während sie sich in Ruhe befinden oder bewegt werden, automatisch. Der Zugriff auf den Data Lake kann auf der Storage Account Ebene über \ac{rbac} gesteuert werden. Für eine granulare Zugriffskontrolle können auf der Verzeichnis/Ordner-Ebene POSIX-Artige Zugriffssteuerungslisten (ACLs) konfiguriert werden.

Mit Hilfe der Dienste \ac{adf} oder Azure Databricks kann der Data Lake um das Open-Source-Layer \textit{Delta Lake} erweitert werden. Durch dieses wird aus dem Data Lake eine ACID-konforme Speicherlösung und die Atomarität, Konsistenz, Isolation und Dauerhaftigkeit der Daten kann garantiert werden \cite{lesteve_definitive_2021}. Der Delta Lake ermöglicht es außerdem deutlich effizienter Personendaten bei Bedarf zu löschen \cite{brown_best_2021}.

\textbf{Fazit}: Die Verwendung von Azure Data Lake Gen 2 ist sinnvoll, wenn viele oder große \ac{blob}-Dateien gespeichert werden müssen. Dies ist zunächst kein Anwendungsfall, der im neuen BI-System benötigt wird. Sollten sich die Anforderungen in Zukunft ändern, könnte Data Lake jedoch eine gute Ergänzung zu einer Datenbank sein.


%===========================================================================================================================
\subsection{Azure Logic Apps} \label{sec:grundlagen:azure_dienste:logicApps}
Mit diesem Dienst können automatisierte Workflows zur Datenintegration erstellt werden. Die Workflows sind sehr anpassungsfähig und es steht eine Vielzahl an vorgefertigten Konnektoren zur Verfügung \cite{kumar_serverless_2019}.

Ein Logic Apps Workflow besteht aus zusammenhängenden Aktivitäten, in einer bestimmten Reihenfolge. Die erste Aktivität ist immer ein Trigger, der die Ausführung des Workflows startet. Alle weiteren Aktivitäten sind Aktionen, die jeweils eine einzige Aufgabe erfüllen und nacheinander abgearbeitet werden. Hier können auch die Konnektoren verwendet werden, mit denen eine Verbindung zu externen Ressourcen aufgebaut werden kann. Die Verbindung ist zu Cloud und on-premise Ressourcen möglich. Die Workflows können über eine Drag-and-Drop-Benutzeroberfläche erstellt werden \cite{modi_azure_2020}.

Auch Datentransformationen, wie mathematische Berechnungen und Textmanipulationen, sind in einem Workflow mit möglich. Dazu werden \ac{dax} Funktionen verwendet, welche mit Excel Formeln vergleichbar sind \cite{bennett_enterprise_2021}.

Beim Eintreten von Fehlern werden vorher festgelegte Wiederholungsrichtlinien ausgeführt. Es ist ebenfalls möglich, nach einem aufgetretenen Fehler, mit einer alternativen Aktion fortzufahren, um zum Beispiel eine zuständige Person per E-Mail zu informieren \cite{fan_handle_2021}.

Für eine ausreichende Sicherheit werden die Daten während der Übertragung mit TLS und im Ruhezustand verschlüsselt. Auch die Authentifizierung mit \ac{aad} und Autorisierung durch \ac{rbac} wird unterstützt \cite{baldwin_azure_logicApps_2021}. 

\textbf{Fazit}: Mit den angebotenen Konnektoren von Logic Apps, können alle hier benötigten Quellsysteme integriert werden \cite[vgl.][]{fan_verwaltete_2021}. Vor dem Speichern können die Daten bei Bedarf verarbeitet werden. Dieser Dienst ist außerdem performant und benötigt für die meisten Ausführungen nur wenige Sekunden \cite{bennett_enterprise_2021}. Durch die Manipulationsmöglichkeiten wäre auch ein Verarbeitungsschritt möglich, in dem nach festen Regeln Attribute zur Datenklassifikation ergänzt werden.


%===========================================================================================================================
\subsection{Azure Data Factory} \label{sec:grundlagen:azure_dienste:dataFactory}
\ac{adf} ist ein Datenintegrationsdienst, der für on-premise und Cloud Systeme genutzt werden kann. Der Dienst wurde speziell für den gemeinsamen Einsatz mit anderen Diensten entwickelt und übernimmt dabei die Bewegung, Transformation und Verarbeitung der Daten, zwischen den unterschiedlichen Systemen \cite{klein_iot_2017}.

Der ETL-Prozess wird als Pipeline implementiert. Eine Pipeline ist eine Abfolge von Aktivitäten zum Bewegen und Transformieren von Daten. Es werden außerdem logische Unterscheidungen unterstützt, mit denen auch auf mögliche Fehler reagiert werden kann \cite{zhu_pipeline_2021}.

Die \textit{Azure Data Factory User Experience} bietet eine webbasierte IDE für die visuelle Erstellung, Planung und Überwachung von Pipelines. Das \textit{Copy Data tool} führt einen schrittweise durch die Erstellung einer neuen Pipeline, die Daten von einer Ressource zu einer anderen verschiebt. Dabei werden unter anderen Datenspeicher- und Computedienste mit der \ac{adf} verknüpft. Die Trennung dieser hat den Vorteil, dass beides unabhängig voneinander nach Bedarf skaliert werden kann.

\ac{adf} besitzt keinen eigenen Speicher, sondern verwendet ausschließlich die verknüpften Dienste. Ein verknüpfter Dienst besteht aus den notwendigen Metadaten für eine Verbindung, beispielsweise die Verbindungszeichenfolge für eine Datenbank. Metadaten, die konkrete Objekte im verknüpften Speicherdienst beschreiben, werden als Dataset definiert. 

Für die Verarbeitung der Daten können verknüpfte Dienste, wie Azure Databricks oder HDInsight verwendet werden. Alternativ bietet eine \textit{integration runtime} Zugang zu internen Rechenressourcen innerhalb von \ac{adf} \cite{swinbank_your_2021}.

Der Zugriff in \ac{adf} kann mit \ac{rbac} beschränkt werden \cite{sabat_security_2021} und die Datenübertragung geschieht verschlüsselt, vorausgesetzt der verbundene Speicherdienst unterstützt TLS oder HTTPS \cite{baldwin_azure_adf_2021}.

\textbf{Fazit}: \ac{adf} könnte im neuen BI-System für die Datenintegration genutzt werden. Im Gegensatz zu Logic Apps setzt dieser Dienst auf die Zusammenarbeit mit weiteren Azure Ressourcen, um den ETL-Prozess durchzuführen, was zu einem komplexeren Gesamtsystem führen könnte.


%===========================================================================================================================
\subsection{Azure HDInsight} \label{sec:grundlagen:azure_dienste:hdInsight}
Azure HDInsight realisiert Apache Hadoop, als vollständig verwalteten Cloud Dienst. Dazu wird von dem Dienst ein Cluster von virtuellen Maschinen, die das \textit{Hadoop Distributed File System} benutzen, verwaltet. Dieses verteilte Dateisystem ist auf Fehlertoleranz und einen hohen Durchsatz ausgelegt ist.

Hadoop ist eine Sammlung von Open~=Source Komponenten, die für die verteilte Verarbeitung und Analyse großer Datensätze konzipiert wurden. Zum Abfragen, Transformieren und Analysieren von Daten stehen unter anderem folgende Funktionen zur Verfügung:
\begin{itemize}
\item \textit{Hive}: Stellt die SQL-artige Abfragesprache HiveQL zur Verfügung und verwendet ein dynamisches Schema, das erst beim Lesen der Daten angewendet wird. Damit kann HDInsight auch ETL-Prozesse durchführen.
\item \textit{Storm}: Echtzeitverarbeitung von großen Datenströmen 
\item \textit{Pig}: Die prozedurale Sprache Pig Latin ermöglicht es Schemata zu erstellen und Abfragen über Skripte durchzuführen.
\item \textit{MapReduce}: Framework zur parallelen Verarbeitung von Daten auf mehreren Rechnern.
\end{itemize}

Hadoop kann eine hohe Verfügbarkeit bieten und kann je nach Bedarf skalieren, wie viele Rechner verwendet werden. Aufgetretene Fehler können selbständig erkannt werden und wenn notwendig wird die fehlgeschlagene Aufgabe an einen anderen Rechner weitergeben \cite{klein_iot_2017}.

\textbf{Fazit}: Dieser Dienst ist vergleichsweise komplex, da individuelle Konfigurationen notwendig sind. Ein möglicher Anwendungsfall wäre ein ETL-Prozess zwischen einem unstrukturierten Speicher, wie dem Data Lake und einer relationalen Datenbank. \cite[vgl.][]{klein_iot_2017}.


%===========================================================================================================================
\subsection{Azure Databricks} \label{sec:grundlagen:azure_dienste:databricks}
Azure Databricks setzt Apache Spark als Cloud-Dienst um. Spark ist ein Open-Source Framework für die Big Data Analyse. Im Gegensatz zu Hadoop werden die Daten zur schnelleren Verarbeitung im Arbeitsspeicher vorgehalten \cite{soh_data_2020}.


%===========================================================================================================================
\subsection{Azure Machine Learning} \label{sec:grundlagen:azure_dienste:machineLearning}
\ac{aml} ist eine Sammlung von Funktionen und Tools zum Arbeiten mit ML-Modellen in einer skalierbaren Cloud-Umgebung. Dafür kann Python, R oder die zero/low code Option \textit{Azure ML designer} verwendet werden.

Von den hier betrachteten Speichermöglichkeiten können entweder Azure SQL-Datenbank oder Data Lake Gen 2 an \ac{aml} angebunden werden \cite{soh_data_2020}.

\textbf{Fazit}: \textit{...}


%===========================================================================================================================
\subsection{Azure Analysis Services} \label{sec:grundlagen:azure_dienste:analysisServices}
\ac{aas} ist eine Cloud Implementierung des Tools \textit{Microsoft SQL Server Analysis Services}. Der Dienst kann als Semantik-Schicht zwischen \ac{dwh} und Endnutzer agieren. In dieser Funktion können beispielsweise Spalten umbenannt oder irrelevante Werte ausgeblendet werden, damit übersichtliche Reports entstehen. Der Dienst kann jedoch nur mit tabellarischen Datenmodellen umgehen, nicht mit multidimensionalen.

Obwohl ein Data Warehouse die gleichen analytischen Möglichkeiten bietet, kann es sinnvoll sein, Speicher- und Analyseschicht für mehr Flexibilität und weniger Komplexität voneinander zu trennen.

Für effiziente Analysen wird das \textit{Vertipaq Engine} genutzt, welches alle Daten im Arbeitsspeicher ablegt. Um den benötigten Speicher zu reduzieren, werden außerdem verschiedene Algorithmen zum Komprimieren der Daten verwendet.

Ein Analysis Services Modell enthält die Daten und weitere Objekte, wie die daraus berechneten \acp{kpi} \cite{how_beyond_2020}.

\ac{aas} verschlüsselt Daten serverseitig und bei der Übertragung und die Implementierung von granularen Zugriffsrichtlinie auf Zeilenniveau mit \ac{rbac} in Verbindung mit dem \ac{aad} ist möglich \cite{duncan_what_2021}.

\textbf{Fazit}: \ac{aas} könnte als Semantik-Schicht zwischen Daten und Reports genutzt werden. Ob der Dienst einen Mehrwert für die Cloud-BI bieten könnte, hängt besonders von der gewählten Speicherlösung und den damit zur Verfügung stehenden Funktionen zur Datenverarbeitung ab.


%===========================================================================================================================
\subsection{Power BI} \label{sec:grundlagen:azure_dienste:powerBI}
Power BI ist Microsofts Dienst zum Visualisieren von Daten. Gleichzeitig können \ac{etl}-Prozesse und Auswertungen durchgeführt werden, die auf der gleichen Power Query Engine, wie bei \ac{aas} basieren. Mit Power BI können vorgefertigte und Self-Service Reports erstellt werden.

Power BI besteht aus mehreren Kernkomponenten wie die Entwicklungsumgebung für Reports \textit{Power BI desktop}, oder das webbasierte Portal \textit{Power BI servcice}, wo die veröffentlichten Reports betrachtet werden können.

Zum Erstellen eines Reports wird zunächst eine Datenquelle verknüpft. Es werden einige Konnektoren angeboten, mit denen unter anderem mit allen hier vorgestellten Speicherlösungen eine Verbindung aufgebaut werden kann. Anschließend kann das dazugehörige Datenmodell bearbeitet werden. Relationen werden automatisch von Power BI erkannt, es können jedoch manuell weitere hinzugefügt werden. Mit einem vollständigen Datenmodell können anschließend Reports mit Visualisierungen, wie Balkendiagramme, in einer grafischen Benutzeroberfläche erstellt werden. Abschließend wird der Report veröffentlicht und wird in Power BI service angezeigt \cite{how_beyond_2020}.

Sollte die Aktualisierung der Reports fehlschlagen, sendet Power BI eine E-Mail, damit zeitnah gehandelt werden kann. Fehler werden auch in einer Übersicht der importierten Datenmodelle angezeigt. Hierüber ist es möglich zu einer Verlaufsübersicht der Aktualisierungen inklusive konkreter Fehlermeldungen zu gelangen \cite{iseminger_data_2021}.

Power BI kann auch eine Liveverbindung zu \ac{aas} aufbauen und Abfragen an diesen Dienst weitergeben, wodurch auf einen vorherigen Import der Daten verzichtet werden kann.
\cite{how_beyond_2020}.

Der Zugriff auf die Daten kann rollenbasiert auf Zeilenebene kontrolliert werden. Sowohl ruhende als auch sich in Bewegung befindende Daten werden verschlüsselt.

Die Ladezeit eines Reports ist von verschiedenen Faktoren wie der Datenquelle, dem Datenmodell und den Visualisierungen abhängig. Daher ist eine allgemeine Aussage zur Performance nicht möglich. Bei Bedarf gibt es können allerdings einige Möglichkeiten zur Optimierung gefunden werden \cite[vgl.][]{myers_optimization_2021}.

\textbf{Fazit}: Mit Power BI können alle hier benötigten Reporting-Funktionalitäten umgesetzte werden, damit ist es eine geeignete Option für das neue BI-System, um als Schnittstelle zu dem Endnutzer zu fungieren. Bezüglich der Performance kann jedoch noch keine Einschätzung gemacht werden. Ob diese zufriedenstellend ist, muss durch eine konkrete Implementierung getestet werden.


%===========================================================================================================================
\subsection{Azure Synapse Analytics} \label{sec:grundlagen:azure_dienste:synapseAnalytics}
Azure Synapse Analytics (ehemals Azure SQL Data Warehouse) vereint Datenintegration, Data Warehouse und Big Data Funktionalität in einem Dienst. Der Dienst enthält eine SQL-Engine und unterstützt weitere Speicherdienste wie den Azure Data Lake. Dadurch können Daten jeglicher Struktur gespeichert und verarbeitet werden. Durch eine unbeschränkte Skalierbarkeit ist dies für mehrere Petabyte möglich.

Die Weboberfläche \textit{Synapse Studio} ist eine einheitliche Verwaltungsschnittstelle, damit Daten über aufgenommen, untersucht, aufbereitet und für BI- und Machine Learining Anwendungen bereitgestellt werden können. Hier sind beispielsweise SQL-Abfragen möglich, die direkte Verwendung von \nameref{sec:grundlagen:azure_dienste:machineLearning}, oder auch die Erstellung von PowerBI Visualisierungen und Reports.

Für die Datenintegration wird die auf \ac{adf} basierende \textit{Synapse Pipeline} angeboten. Dadurch ist eine einfache Verbindung zu einer Vielzahl von Quellsystemen möglich.

Durch Synapse Spark, welches auf Grundlage von Apache Spark entwickelt wurde, wird die Analyse von großen Datenmengen ermöglicht. Hierfür kann Python, Scala, C\# oder Spark SQL verwendet werden.

Azure Synapse Analytics bietet einige Sicherheitsfunktionen, darunter die automatische Erkennung von Bedrohungen und eine immer aktive Verschlüsselung. Der Zugriff kann auf Spalten- oder Zeilenlevel geregelt werden.
\cite{shiyal_beginning_2021}

\textbf{Fazit}: Azure Synapse Analytics vereint mehrere Technologien und andere Azure Dienste in einem, wodurch die meisten funktionalen Anforderungen an das Cloud-BI-System erfüllt werden könnten. Jedoch ist dieser Dienst vergleichsweise komplex. Es werden einige Funktionalitäten geboten, die besonders für Big Data Anwendungsfälle attraktiv sind, jedoch hier nicht benötigt werden.


%===========================================================================================================================
\subsection{Azure Purview} \label{sec:grundlagen:azure_dienste:purview}
Purview dient der zentralen Verwaltung der Datengovernance, für Cloud und on-premise Umgebungen. Dabei soll die Datenherkunft leicht nachvollziehbar werden. Die Metadaten zu den Quellen können um Tags und Beschreibungen erweitert werden. Der Dienst kann über die Benutzeroberfläche Purview Studio verwaltet werden. Der Zugriff auf dieses und die Informationen wird über Rollen und das \ac{aad} kontrolliert.

Im Purview Studio können Benutzer neue Datenquellen registriert werden, damit zusätzliche Metadaten zu diesen verwaltet werden können. Durch das Erstellen einer \textit{Scan Rule} können die Datenquellen gescannt werden, wobei die Daten nach vorher definierten Regeln klassifiziert werden. Der Scan kann einmalig oder wiederholt nach Zeitplan ausgeführt werden \cite{lesteve_definitive_2021}. Dadurch wird die Nachvollziehbarkeit der Datenherkunft ermöglicht. Diese umfasst den Ursprung, aber auch alle Verarbeitungen und Bewegungen der Daten \cite{riscutia_data_2021}.

Auch die Übertragung und Speicherung der Daten geschieht in Purview verschlüsselt \cite{baldwin_azure_purview_2021}. 

\textbf{Fazit}: Purview bietet einige Funktionen, die helfen, die Sicherheit und den Datenschutz der BI-Lösung zu verbessern. Purview kann verwendet werden, um die Daten in regelmäßigen Scans zu klassifizieren und dabei den vollständigen Datenfluss zu dokumentieren. Durch die Erweiterung der Metadaten um bestimmte Tags kann Purview zusätzlich dabei helfen, auf Anfrage alle Daten eines Kunden zu löschen.